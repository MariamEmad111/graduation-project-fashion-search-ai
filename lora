!pip install datasets peft accelerate bitsandbytes --quiet

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from datasets import Dataset


df = pd.read_csv('/content/drive/MyDrive/clothing_dataset_ar_en_translated.csv')

dataset = Dataset.from_pandas(df)
dataset = dataset.train_test_split(test_size=0.1)

!pip uninstall -y torch torchvision torchaudio
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

from transformers import MBart50TokenizerFast, MBartForConditionalGeneration

model_path = "/content/drive/MyDrive/mBART/mBART"

tokenizer = MBart50TokenizerFast.from_pretrained(model_path, local_files_only=True)
model = MBartForConditionalGeneration.from_pretrained(model_path, local_files_only=True)

max_len = 64

def preprocess(examples):
    inputs = tokenizer(
        examples['Arabic'],
        max_length=max_len,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    targets = tokenizer(
        examples['English'],
        max_length=max_len,
        padding="max_length",
        truncation=True
    )

    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized = dataset.map(preprocess, batched=True, remove_columns=dataset['train'].column_names)

from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
import torch

model = prepare_model_for_kbit_training(model)

config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, config)

!pip install -U transformers

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

training_args = Seq2SeqTrainingArguments(
    output_dir="./mbart50-lora-fashion",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-4,
    num_train_epochs=5,
    #evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    fp16=True,
    save_total_limit=2,
    report_to="none"
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
)

trainer.train()

trainer.save_model("/content/drive/MyDrive/mbart50_finetuned_lora")

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model_path = "/content/drive/MyDrive/mbart50_finetuned_lora"


tokenizer = MBart50TokenizerFast.from_pretrained(model_path)
model = MBartForConditionalGeneration.from_pretrained(model_path)


input_text = "فستان أحمر ستان من بريشكا"


inputs = tokenizer(input_text, return_tensors="pt")


generated_tokens = model.generate(
    **inputs,
    forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"],
    max_length=50,
    num_beams=4,
    early_stopping=True
)


output_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)

print("Arabic:", input_text)
print("English:", output_text)

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model_path = "/content/drive/MyDrive/mbart50_finetuned_lora"


tokenizer = MBart50TokenizerFast.from_pretrained(model_path)
model = MBartForConditionalGeneration.from_pretrained(model_path)


!pip install evaluate sacrebleu


from datasets import load_dataset, load_metric
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
import torch
from tqdm import tqdm
import evaluate


tokenizer.src_lang = "ar_AR"

# 4.Batching
def compute_bleu_batch(model, tokenizer, dataset, max_len=64, batch_size=16):
    metric = evaluate.load("sacrebleu")
    predictions = []
    references = []

    for i in tqdm(range(0, len(dataset), batch_size)):
        batch = dataset[i:i+batch_size]
        input_texts = batch['Arabic']
        target_texts = batch['English']

        inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_len).to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"],
                max_length=max_len
            )
        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        predictions.extend(decoded_preds)
        references.extend([[ref] for ref in target_texts])  # BLEU expects list of list

    bleu = metric.compute(predictions=predictions, references=references)
    return bleu

# 5. test
sample_dataset = dataset['test'].select(range(1000))
results = compute_bleu_batch(model, tokenizer, sample_dataset, batch_size=16)
print(f"\n✅ BLEU score: {results['score']:.2f}")
